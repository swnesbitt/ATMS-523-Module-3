{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 N1 Introduction to bokeh with pandas\n",
    "\n",
    "In this notebook, we'll introduce the `bokeh` plotting package in `python`, wrangle some cloud-hosted data with `pandas`, and introduce some of the methods you'll use in Module 3's homework.\n",
    "\n",
    "You'll need to install a few new packages for this assignment, so be sure to install them in the class conda environment.\n",
    "\n",
    "One is `pandas-bokeh`, which can be installed with\n",
    "\n",
    "```bash\n",
    "pip install pandas-bokeh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, for this notebook, and for your assignment, we'll be using the NOAA Global Historical Climatology Network Daily (GHCN-D) database, which is the official record for daily weather observations.  The dataset is produced by the National Centers for Environmental Information https://doi.org/10.7289/V5D21VHZ.  As part of the NOAA Big Data project, this dataset is hosted in the cloud by several popular cloud data services. For this assignment, we'll use the service on Amazon Web Services, who hosts the dataset in an S3 bucket in a few formats. The link to the S3 page for GHCN-D is https://registry.opendata.aws/noaa-ghcn/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is produced in its original format in fixed-width format text files.  We'll use `pandas` to load some metadata - the station inventory file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_ids = pd.read_fwf('http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt', header=None, infer_nrows=1000)\n",
    "stn_ids.columns = ['ID','LAT','LON','ELEV','UKN','NAME','GSN','WBAN']\n",
    "stn_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next file we'll open is the station inventory file.  It describes for each site and each variable, the years that each variable is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = pd.read_fwf('http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-inventory.txt', header=None, infer_nrows=1000)\n",
    "periods.columns = ['ID','LAT','LON','ELEM','TiMIN','TiMAX']\n",
    "periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `pandas` to merge these two tables into one based on the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stns = pd.merge(stn_ids,periods,how='left',left_on='ID',right_on='ID')\n",
    "merged_stns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the merged table just for the `TMAX` element, and those stations still reporting in 2022.  Notice how the number of stations drops - most of the stations in the dataset measure rainfall & many stations have stopped reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset for T\n",
    "merged_stns = merged_stns[(merged_stns['ELEM'] == 'TMAX') & (merged_stns['TiMAX'] == 2022)]\n",
    "merged_stns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize where these currently reporting stations are using a map, depending on when they started taking measurements.  We'll sort them so the oldest ones are last, so they plot on the top of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stns_sorted = merged_stns.sort_values('TiMIN', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stns_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a scatter plot on a `cartopy` map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from cartopy import crs as ccrs\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "# make the map global rather than have it zoom in to\n",
    "# the extents of any plotted data\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "\n",
    "plt.scatter(merged_stns_sorted['LON_x'],merged_stns_sorted['LAT_x'],0.25,c=merged_stns_sorted['TiMIN'], \n",
    "            transform=ccrs.PlateCarree(), cmap='magma')\n",
    "plt.colorbar(shrink = 0.5, label='year')\n",
    "plt.title('Year of first TMAX measurement in GHCN daily data with stations still reporting in 2022');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a query to find stations that are reporting in 2022 and contain the string 'CHAMPAIGN' the station name field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stns[merged_stns['NAME'].str.contains('CHAMPAIGN', regex=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data! The data is avaiable in AWS in a couple of formats.  It has a mirror of the raw data format in column separated values (csv), and pandas can read it without a problem.  I've added a couple of flags to avoid errors, but this should work to open the data in this format.\n",
    "\n",
    "Again, to learn more, visit the AWS site for GHCN-D (https://registry.opendata.aws/noaa-ghcn/)\n",
    "\n",
    "Let's load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "...      \"s3://noaa-ghcn-pds/csv/by_station/USC00118740.csv\",\n",
    "...      storage_options={\"anon\": True},  # passed to `s3fs.S3FileSystem`\n",
    "         dtype={'Q_FLAG': 'object', 'M_FLAG': 'object'},\n",
    "         parse_dates=['DATE']\n",
    "... ).set_index('DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure of the data, each \"element\" takes up one line, there are QC flags for each variables, and other informtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the data in a parquet format, which is a cloud-optimized database format.  It also works well with `dask`.  We won't use `dask` for this exercise because a single site is a small file.  However, if you were interested in looking at larger portions of the dataset, `dask` could help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    \"s3://noaa-ghcn-pds/parquet/by_station/STATION=USC00118740/\",\n",
    "    storage_options={\"anon\": True},  # passed to `s3fs.S3FileSystem`\n",
    ")\n",
    "\n",
    "#make date the index\n",
    "df['DATE'] = df['DATE'].apply(lambda x: datetime.strptime(x, '%Y%m%d'))\n",
    "df = df.set_index('DATE').sort_index()  #we need to sort by time because the files are sorted to be arbitrary\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create separate dataframes for elements of interest: Maximum and Minimum Temperature and Precipitaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmax = df.loc[df['ELEMENT'] == 'TMAX']\n",
    "df_tmin = df.loc[df['ELEMENT'] == 'TMIN']\n",
    "df_prcp = df.loc[df['ELEMENT'] == 'PRCP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values are a not a magnitude you might expect - they are scaled integers.  To get values in degrees C, divide the values by 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh and pandas plotting\n",
    "\n",
    "So, let's visualize this data using `bokeh`.  `bokeh` is an alternative to `matplotlib` and it produces interactive high quality plots that can be visualized in the notebook or on a website.  Check out the `bokeh` website to see what it can do: (https://bokeh.org).\n",
    "\n",
    "First, we'll use the pandas_bokeh library to plot directly from the pandas dataframe.  We'll also have the plots shown in the notebook here, but you should try turning that off and seeing what happens.  If successful, you should see a little badge below the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_bokeh\n",
    "\n",
    "# see what happens when you comment this out and try to make a bokeh plot!\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's plot the time series of maximum temperature for the station we loaded above.  We'll add a fancy rangetool to show the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_tmax['DATA_VALUE']/10.).plot_bokeh(rangetool=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an annual mean minimum temperature time series using `pandas` groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser=df_tmin[~((df_tmin.index.month==2)&(df_tmin.index.day==29))]\n",
    "\n",
    "bk_bar1 = (ser['DATA_VALUE']/10.).groupby(ser.index.day_of_year).mean().plot_bokeh().line()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to create basing plots using `pandas_bokeh`, but you can also using the `bokeh` interface to make plots similar to a matplotlib-type plot.  Here, let's show the maximum temperatures for 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bokeh basics\n",
    "from turtle import color\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "# Create the blank plot\n",
    "p = figure(plot_height = 600, plot_width = 600, \n",
    "           title = 'Time series of maximum temperatures - 2022',\n",
    "          x_axis_label = 'Time', \n",
    "           y_axis_label = 'Temperature (Â°C)',\n",
    "           x_axis_type='datetime')\n",
    "\n",
    "df_2022 = df_tmax[df_tmax.index >= '2022-01-01']\n",
    "p.line(df_2022.index, df_2022['DATA_VALUE']/10., legend_label=\"TMAX\", line_width=2)\n",
    "df_2_2022 = df_tmin[df_tmin.index >= '2022-01-01']\n",
    "p.line(df_2_2022.index, df_2_2022['DATA_VALUE']/10., legend_label=\"TMIN\", line_width=2, color='orange')\n",
    "\n",
    "# Show the plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a histogram of low temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr_hist, edges = np.histogram(df_tmin['DATA_VALUE'][(df_tmin.index >= '1991-01-01') & (df_tmin.index < '2020-01-01')]/10., \n",
    "                               bins = np.arange(-40,47,2.5), density=True)\n",
    "# Put the information in a dataframe\n",
    "temps = pd.DataFrame({'hist': arr_hist, \n",
    "                       'left': edges[:-1], \n",
    "                       'right': edges[1:]})\n",
    "\n",
    "arr_hist, edges = np.histogram(df_tmin['DATA_VALUE'][(df_tmin.index >= '1901-01-01') & (df_tmin.index < '1930-01-01')]/10., \n",
    "                               bins = np.arange(-40,47,2.5), density=True)\n",
    "# Put the information in a dataframe\n",
    "temps2 = pd.DataFrame({'hist': arr_hist, \n",
    "                       'left': edges[:-1], \n",
    "                       'right': edges[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bokeh basics\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "# Create the blank plot\n",
    "p = figure(plot_height = 600, plot_width = 600, \n",
    "           title = 'Histogram of Low Temperatures at CHAMPAIGN 3S',\n",
    "          x_axis_label = 'Temperature (Â°C)', \n",
    "           y_axis_label = 'Fraction of Days')\n",
    "\n",
    "# Add a quad glyph\n",
    "p.quad(bottom=0, top=temps['hist'], \n",
    "       left=temps['left'], right=temps['right'], \n",
    "       fill_color='red', line_color='black', alpha=0.5,legend_label=\"1991-2020\")\n",
    "\n",
    "p.quad(bottom=0, top=temps2['hist'], \n",
    "       left=temps2['left'], right=temps2['right'], \n",
    "       fill_color='blue', line_color='black', alpha=0.5, legend_label=\"1901-1930\")\n",
    "\n",
    "# Show the plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47315f6e84c884cb52603c51d435ec20639e4089a0edf54cccfd4b524eb840ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
